{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=\"./data/train.txt\"\n",
    "vali_path=\"./data/vali.txt\"\n",
    "test_path=\"./data/test.txt\"\n",
    "output_dir=\"./output\"\n",
    "train_batch_size= 32\n",
    "num_train_steps= 100\n",
    "learning_rate= 0.01\n",
    "dropout_rate= 0.5\n",
    "hidden_layer_dims=[\"16\", \"8\", \"4\"]\n",
    "num_features=136\n",
    "list_size= 100\n",
    "group_size= 1\n",
    "loss=\"pairwise_logistic_loss\"\n",
    "secondary_loss= None\n",
    "secondary_loss_weight= 0.5\n",
    "_PRIMARY_HEAD = \"primary_head\"\n",
    "_SECONDARY_HEAD = \"secondary_head\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IteratorInitializerHook(tf.estimator.SessionRunHook):\n",
    "    # Hook to initialize data iterator after session is created.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(IteratorInitializerHook, self).__init__()\n",
    "        self.iterator_initializer_fn = None\n",
    "    def after_create_session(self, session, coord):\n",
    "        \"\"\"Initialize the iterator after the session has been created.\"\"\"\n",
    "        del coord\n",
    "        self.iterator_initializer_fn(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _use_multi_head():\n",
    "  # Returns True if using multi-head.\"\"\"\n",
    "  return secondary_loss is not None\n",
    "def example_feature_columns():\n",
    "    #Returns the example feature columns.\"\"\"\n",
    "    feature_names = [\"{}\".format(i + 1) for i in range(num_features)]\n",
    "    return {name:tf.feature_column.numeric_column(name, shape=(1,), default_value=0.0)for name in feature_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load_libsvm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_libsvm_data(path, list_size):\n",
    "    # Returns features and labels in numpy.array.\"\"\"\n",
    "    def _parse_line(line):\n",
    "        #Parses a single line in LibSVM format.\"\"\"\n",
    "        tokens = line.split(\"#\")[0].split()\n",
    "        assert len(tokens) >= 2, \"Ill-formatted line: {}\".format(line)\n",
    "        label = float(tokens[0])\n",
    "        qid = tokens[1]\n",
    "        kv_pairs = [kv.split(\":\") for kv in tokens[2:]]\n",
    "        features = {k: float(v) for (k, v) in kv_pairs}\n",
    "        return qid, features, label\n",
    "    # The 0-based index assigned to a query.\n",
    "    qid_to_index = {}\n",
    "    # The number of docs seen so far for a query.\n",
    "    qid_to_ndoc = {}\n",
    "    # Each feature is mapped an array with [num_queries, list_size, 1]. Label has\n",
    "    # a shape of [num_queries, list_size]. We use list for each of them due to the\n",
    "    # unknown number of queries.\n",
    "    feature_map = {k: [] for k in example_feature_columns()}\n",
    "    label_list = []\n",
    "    total_docs = 0\n",
    "    discarded_docs = 0\n",
    "    with open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            qid, features, label = _parse_line(line)\n",
    "            if qid not in qid_to_index:\n",
    "                # Create index and allocate space for a new query.\n",
    "                qid_to_index[qid] = len(qid_to_index)\n",
    "                qid_to_ndoc[qid] = 0\n",
    "                for k in feature_map:\n",
    "                    feature_map[k].append(np.zeros([list_size, 1], dtype=np.float32))\n",
    "            label_list.append(np.ones([list_size], dtype=np.float32) * -1.)\n",
    "            total_docs += 1\n",
    "            batch_idx = qid_to_index[qid]\n",
    "            doc_idx = qid_to_ndoc[qid]\n",
    "            qid_to_ndoc[qid] += 1\n",
    "            # Keep the first 'list_size' docs only.\n",
    "            if doc_idx >= list_size:\n",
    "                discarded_docs += 1\n",
    "                continue\n",
    "            for k, v in six.iteritems(features):\n",
    "                assert k in feature_map, \"Key {} not found in features.\".format(k)\n",
    "                feature_map[k][batch_idx][doc_idx, 0] = v\n",
    "            label_list[batch_idx][doc_idx] = label\n",
    "          # Convert everything to np.array.\n",
    "        tf.compat.v1.logging.info(\"Number of queries: {}\".format(len(qid_to_index)))\n",
    "        tf.compat.v1.logging.info(\"Number of documents in total: {}\".format(total_docs))\n",
    "        tf.compat.v1.logging.info(\"Number of documents discarded: {}\".format(discarded_docs))\n",
    "        for k in feature_map:\n",
    "            feature_map[k] = np.array(feature_map[k])    \n",
    "    return feature_map, np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Number of queries: 27\n",
      "INFO:tensorflow:Number of documents in total: 119\n",
      "INFO:tensorflow:Number of documents discarded: 0\n",
      "len(loaded_data) 2\n",
      "len(loaded_data[0]) : 136\n",
      "len(loaded_data[1]) : 119\n",
      "len(loaded_data[0]['1']) : 27\n",
      "len(loaded_data[0]['1'][0]) : 100\n"
     ]
    }
   ],
   "source": [
    "train_path=\"./data/train.txt\"\n",
    "list_size=100\n",
    "loaded_data=load_libsvm_data(train_path, list_size) # loaded_data is a tuple\n",
    "print(\"len(loaded_data)\",len(loaded_data))\n",
    "print(\"len(loaded_data[0]) :\",len(loaded_data[0]))\n",
    "print(\"len(loaded_data[1]) :\",len(loaded_data[1]))\n",
    "print(\"len(loaded_data[0]['1']) :\",len(loaded_data[0]['1']))\n",
    "print(\"len(loaded_data[0]['1'][0]) :\",len(loaded_data[0]['1'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inputs for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_inputs(features, labels, batch_size): # Set up training input in batches.\n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "    def _train_input_fn(): # Defines training input fn\n",
    "        features_placeholder = {\n",
    "            k: tf.compat.v1.placeholder(v.dtype, v.shape)\n",
    "            for k, v in six.iteritems(features)\n",
    "        }\n",
    "        if _use_multi_head():\n",
    "            placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "            labels_placeholder = {_PRIMARY_HEAD: placeholder,_SECONDARY_HEAD: placeholder,}\n",
    "        else:\n",
    "            labels_placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (features_placeholder, labels_placeholder))\n",
    "        dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "        iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "        if _use_multi_head():\n",
    "            feed_dict = {labels_placeholder[head_name]: labels for head_name in labels_placeholder}\n",
    "        else:\n",
    "            feed_dict = {labels_placeholder: labels}\n",
    "        feed_dict.update({features_placeholder[k]: features[k] for k in features_placeholder})\n",
    "        iterator_initializer_hook.iterator_initializer_fn = (lambda sess: sess.run(iterator.initializer, feed_dict=feed_dict))\n",
    "        return iterator.get_next()\n",
    "    return _train_input_fn, iterator_initializer_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Define Inputs for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_inputs(features, labels): # Set up eval inputs in a single batch.\"\"\"\n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "    def _eval_input_fn(): #Defines eval input fn.\n",
    "        features_placeholder = { k: tf.compat.v1.placeholder(v.dtype, v.shape) for k, v in six.iteritems(features)}\n",
    "        if _use_multi_head():\n",
    "            placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "            labels_placeholder = {_PRIMARY_HEAD: placeholder,_SECONDARY_HEAD: placeholder,}\n",
    "        else:\n",
    "            labels_placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "        dataset = tf.data.Dataset.from_tensors((features_placeholder, labels_placeholder))\n",
    "        iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "        if _use_multi_head():\n",
    "            feed_dict = {labels_placeholder[head_name]: labels for head_name in labels_placeholder}\n",
    "        else:\n",
    "            feed_dict = {labels_placeholder: labels}\n",
    "        feed_dict.update({features_placeholder[k]: features[k] for k in features_placeholder})\n",
    "        iterator_initializer_hook.iterator_initializer_fn = (lambda sess: sess.run(iterator.initializer, feed_dict=feed_dict))\n",
    "        return iterator.get_next()\n",
    "    return _eval_input_fn, iterator_initializer_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Number of queries: 27\n",
      "INFO:tensorflow:Number of documents in total: 119\n",
      "INFO:tensorflow:Number of documents discarded: 0\n",
      "INFO:tensorflow:Number of queries: 9\n",
      "INFO:tensorflow:Number of documents in total: 36\n",
      "INFO:tensorflow:Number of documents discarded: 0\n",
      "INFO:tensorflow:Number of queries: 9\n",
      "INFO:tensorflow:Number of documents in total: 36\n",
      "INFO:tensorflow:Number of documents discarded: 0\n"
     ]
    }
   ],
   "source": [
    "features, labels = load_libsvm_data(train_path, list_size)\n",
    "train_input_fn, train_hook = get_train_inputs(features, labels,train_batch_size)\n",
    "features_vali, labels_vali = load_libsvm_data(vali_path,list_size)\n",
    "vali_input_fn, vali_hook = get_eval_inputs(features_vali, labels_vali)\n",
    "features_test, labels_test = load_libsvm_data(test_path,list_size)\n",
    "test_input_fn, test_hook = get_eval_inputs(features_test, labels_test)\n",
    "optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_serving_input_fn(): # Returns serving input fn to receive tf.Example.\"\"\"\n",
    "    feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "        example_feature_columns().values())\n",
    "    return tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "        feature_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform_fn(): # Returns a transform_fn that converts features to dense Tensors\n",
    "  def _transform_fn(features, mode): # Defines transform_fn\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      # We expect tf.Example as input during serving. In this case, group_size\n",
    "      # must be set to 1.\n",
    "      if group_size != 1:\n",
    "        raise ValueError(\n",
    "            \"group_size should be 1 to be able to export model, but get %s\" %\n",
    "            group_size)\n",
    "      context_features, example_features = (\n",
    "          tfr.feature.encode_pointwise_features(\n",
    "              features=features,\n",
    "              context_feature_columns=None,\n",
    "              example_feature_columns=example_feature_columns(),\n",
    "              mode=mode,\n",
    "              scope=\"transform_layer\"))\n",
    "    else:\n",
    "      context_features, example_features = tfr.feature.encode_listwise_features(\n",
    "          features=features,\n",
    "          context_feature_columns=None,\n",
    "          example_feature_columns=example_feature_columns(),\n",
    "          mode=mode,\n",
    "          scope=\"transform_layer\")\n",
    "\n",
    "    return context_features, example_features\n",
    "\n",
    "  return _transform_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make_Score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score_fn():\n",
    "  \"\"\"Returns a groupwise score fn to build `EstimatorSpec`.\"\"\"\n",
    "\n",
    "  def _score_fn(unused_context_features, group_features, mode, unused_params,\n",
    "                unused_config):\n",
    "    \"\"\"Defines the network to score a group of documents.\"\"\"\n",
    "    with tf.compat.v1.name_scope(\"input_layer\"):\n",
    "      group_input = [\n",
    "          tf.compat.v1.layers.flatten(group_features[name])\n",
    "          for name in sorted(example_feature_columns())\n",
    "      ]\n",
    "      input_layer = tf.concat(group_input, 1)\n",
    "      tf.compat.v1.summary.scalar(\"input_sparsity\",\n",
    "                                  tf.nn.zero_fraction(input_layer))\n",
    "      tf.compat.v1.summary.scalar(\"input_max\",\n",
    "                                  tf.reduce_max(input_tensor=input_layer))\n",
    "      tf.compat.v1.summary.scalar(\"input_min\",\n",
    "                                  tf.reduce_min(input_tensor=input_layer))\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    cur_layer = tf.compat.v1.layers.batch_normalization(\n",
    "        input_layer, training=is_training)\n",
    "    for i, layer_width in enumerate(int(d) for d in hidden_layer_dims):\n",
    "      cur_layer = tf.compat.v1.layers.dense(cur_layer, units=layer_width)\n",
    "      cur_layer = tf.compat.v1.layers.batch_normalization(\n",
    "          cur_layer, training=is_training)\n",
    "      cur_layer = tf.nn.relu(cur_layer)\n",
    "      tf.compat.v1.summary.scalar(\"fully_connected_{}_sparsity\".format(i),\n",
    "                                  tf.nn.zero_fraction(cur_layer))\n",
    "    cur_layer = tf.compat.v1.layers.dropout(\n",
    "        cur_layer, rate=dropout_rate, training=is_training)\n",
    "    logits = tf.compat.v1.layers.dense(cur_layer, units=group_size)\n",
    "    if _use_multi_head():\n",
    "      # Duplicate the logits for both heads.\n",
    "      return {_PRIMARY_HEAD: logits, _SECONDARY_HEAD: logits}\n",
    "    else:\n",
    "      return logits\n",
    "\n",
    "  return _score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metric_fns():\n",
    "  \"\"\"Returns a dict from name to metric functions.\"\"\"\n",
    "  metric_fns = {}\n",
    "  metric_fns.update({\n",
    "      \"metric/%s\" % name: tfr.metrics.make_ranking_metric_fn(name) for name in [\n",
    "          tfr.metrics.RankingMetricKey.ARP,\n",
    "          tfr.metrics.RankingMetricKey.ORDERED_PAIR_ACCURACY,\n",
    "      ]\n",
    "  })\n",
    "  metric_fns.update({\n",
    "      \"metric/ndcg@%d\" % topn: tfr.metrics.make_ranking_metric_fn(\n",
    "          tfr.metrics.RankingMetricKey.NDCG, topn=topn)\n",
    "      for topn in [1, 3, 5, 10]\n",
    "  })\n",
    "  return metric_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features[\"3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building groupwise ranking model.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function _make_model_fn.<locals>._model_fn at 0x0000016D7117F558>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "def _train_op_fn(loss):\n",
    "    \"\"\"Defines train op used in ranking head.\"\"\"\n",
    "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "    minimize_op = optimizer.minimize(\n",
    "        loss=loss, global_step=tf.compat.v1.train.get_global_step())\n",
    "    train_op = tf.group([minimize_op, update_ops])\n",
    "    return train_op\n",
    "if _use_multi_head():\n",
    "    primary_head = tfr.head.create_ranking_head(loss_fn=tfr.losses.make_loss_fn(loss),eval_metric_fns=get_eval_metric_fns(),train_op_fn=_train_op_fn,name=_PRIMARY_HEAD)\n",
    "    secondary_head = tfr.head.create_ranking_head(loss_fn=tfr.losses.make_loss_fn(secondary_loss),eval_metric_fns=get_eval_metric_fns(),train_op_fn=_train_op_fn,name=_SECONDARY_HEAD)\n",
    "    ranking_head = tfr.head.create_multi_ranking_head([primary_head, secondary_head], [1.0, secondary_loss_weight])\n",
    "else:\n",
    "    ranking_head = tfr.head.create_ranking_head(loss_fn=tfr.losses.make_loss_fn(loss),eval_metric_fns=get_eval_metric_fns(),train_op_fn=_train_op_fn)\n",
    "\n",
    "#create the estimator\n",
    "mymodel_fn=tfr.model.make_groupwise_ranking_fn(group_score_fn=make_score_fn(),group_size=group_size,transform_fn=make_transform_fn(),ranking_head=ranking_head)\n",
    "myconfig=tf.estimator.RunConfig(output_dir, save_checkpoints_steps=100)\n",
    "estimator = tf.estimator.Estimator(model_fn=mymodel_fn,config=myconfig)\n",
    "\n",
    "#create the specs\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn,hooks=[train_hook],max_steps=num_train_steps)\n",
    "if group_size == 1: # Export model to accept tf.Example when group_size = 1.\n",
    "    vali_spec = tf.estimator.EvalSpec(input_fn=vali_input_fn,hooks=[vali_hook],steps=1,exporters=tf.estimator.LatestExporter(\"latest_exporter\",serving_input_receiver_fn=make_serving_input_fn()),start_delay_secs=0,throttle_secs=30)\n",
    "else:\n",
    "    vali_spec = tf.estimator.EvalSpec(input_fn=vali_input_fn,hooks=[vali_hook],steps=1,start_delay_secs=0,throttle_secs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.train_and_evaluate(estimator, train_spec, vali_spec) # Train and validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Get_Eval_Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_inputs(features, labels): # Set up eval inputs in a single batch.\"\"\"\n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "    def _eval_input_fn(): #Defines eval input fn.\n",
    "        features_placeholder = { k: tf.compat.v1.placeholder(v.dtype, v.shape) for k, v in six.iteritems(features)}\n",
    "        if _use_multi_head():\n",
    "            placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "            labels_placeholder = {_PRIMARY_HEAD: placeholder,_SECONDARY_HEAD: placeholder,}\n",
    "        else:\n",
    "            labels_placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "        dataset = tf.data.Dataset.from_tensors((features_placeholder, labels_placeholder))\n",
    "        iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "        if _use_multi_head():\n",
    "            feed_dict = {labels_placeholder[head_name]: labels for head_name in labels_placeholder}\n",
    "        else:\n",
    "            feed_dict = {labels_placeholder: labels}\n",
    "        feed_dict.update({features_placeholder[k]: features[k] for k in features_placeholder})\n",
    "        iterator_initializer_hook.iterator_initializer_fn = (lambda sess: sess.run(iterator.initializer, feed_dict=feed_dict))\n",
    "        return iterator.get_next()\n",
    "    return _eval_input_fn, iterator_initializer_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:329: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n",
      "E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\normalization.py:308: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  '`tf.layers.batch_normalization` is deprecated and '\n",
      "E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:268: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
      "  warnings.warn('`tf.layers.dropout` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-01-05T12:25:34Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./output\\model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[35,99,0] = [35, 0] does not index into param shape [9,100,1]\n\t [[node groupwise_dnn_v2/group_features/GatherNd_95 (defined at E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py:374) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node groupwise_dnn_v2/group_features/GatherNd_95:\n transform/Reshape_231 (defined at E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\utils.py:179)\t\n groupwise_dnn_v2/concat/concat (defined at E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py:336)\n\nOriginal stack trace for 'groupwise_dnn_v2/group_features/GatherNd_95':\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2878, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3147, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-1bc7eaf82f0e>\", line 2, in <module>\n    evaluation_report=estimator.evaluate(input_fn=test_input_fn, hooks=[test_hook]);\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 467, in evaluate\n    name=name)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 510, in _actual_eval\n    return _evaluate()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 492, in _evaluate\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1531, in _evaluate_build_graph\n    self._call_model_fn_eval(input_fn, self.config))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1567, in _call_model_fn_eval\n    config)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1163, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py\", line 446, in _model_fn\n    config)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py\", line 132, in compute_logits\n    labels, mode, params, config)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py\", line 374, in _compute_logits_impl\n    value = tf.gather_nd(value, self._feature_gather_indices)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 5143, in gather_nd_v2\n    return gather_nd(params, indices, name=name, batch_dims=batch_dims)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 5135, in gather_nd\n    return gen_array_ops.gather_nd(params, indices, name=name)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3704, in gather_nd\n    \"GatherNd\", params=params, indices=indices, name=name)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 750, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3536, in _create_op_internal\n    op_def=op_def)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1990, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1360\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1453\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[35,99,0] = [35, 0] does not index into param shape [9,100,1]\n\t [[{{node groupwise_dnn_v2/group_features/GatherNd_95}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1bc7eaf82f0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_input_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_eval_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mevaluation_report\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_input_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_hook\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m   def _actual_eval(self,\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[1;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[0meval_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0mall_hooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             output_dir=self.eval_dir(name))\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_evaluate_run\u001b[1;34m(self, checkpoint_path, scaffold, update_op, eval_dict, all_hooks, output_dir)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         \u001b[0mfinal_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1646\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1647\u001b[1;33m         config=self._session_config)\n\u001b[0m\u001b[0;32m   1648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m     \u001b[0mcurrent_global_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGLOBAL_STEP\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\training\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[1;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Inference Time : {:0.5f}s'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m         run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1281\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m             run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1284\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         logging.info(\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 968\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1191\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1369\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1392\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1394\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1396\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[35,99,0] = [35, 0] does not index into param shape [9,100,1]\n\t [[node groupwise_dnn_v2/group_features/GatherNd_95 (defined at E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py:374) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node groupwise_dnn_v2/group_features/GatherNd_95:\n transform/Reshape_231 (defined at E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\utils.py:179)\t\n groupwise_dnn_v2/concat/concat (defined at E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py:336)\n\nOriginal stack trace for 'groupwise_dnn_v2/group_features/GatherNd_95':\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2878, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3147, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-1bc7eaf82f0e>\", line 2, in <module>\n    evaluation_report=estimator.evaluate(input_fn=test_input_fn, hooks=[test_hook]);\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 467, in evaluate\n    name=name)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 510, in _actual_eval\n    return _evaluate()\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 492, in _evaluate\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1531, in _evaluate_build_graph\n    self._call_model_fn_eval(input_fn, self.config))\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1567, in _call_model_fn_eval\n    config)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1163, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py\", line 446, in _model_fn\n    config)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py\", line 132, in compute_logits\n    labels, mode, params, config)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow_ranking\\python\\model.py\", line 374, in _compute_logits_impl\n    value = tf.gather_nd(value, self._feature_gather_indices)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 5143, in gather_nd_v2\n    return gather_nd(params, indices, name=name, batch_dims=batch_dims)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 5135, in gather_nd\n    return gen_array_ops.gather_nd(params, indices, name=name)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3704, in gather_nd\n    \"GatherNd\", params=params, indices=indices, name=name)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 750, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3536, in _create_op_internal\n    op_def=op_def)\n  File \"E:\\00_Installed_programs\\anaconda\\envs\\tensorflow_2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1990, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "test_input_fn, test_hook = get_eval_inputs(features_test, labels_test)\n",
    "evaluation_report=estimator.evaluate(input_fn=test_input_fn, hooks=[test_hook]); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_inputs(features, labels):\n",
    "  \"\"\"Set up eval inputs in a single batch.\"\"\"\n",
    "  iterator_initializer_hook = IteratorInitializerHook()\n",
    "\n",
    "  def _eval_input_fn():\n",
    "    \"\"\"Defines eval input fn.\"\"\"\n",
    "    features_placeholder = {\n",
    "        k: tf.compat.v1.placeholder(v.dtype, v.shape)\n",
    "        for k, v in six.iteritems(features)\n",
    "    }\n",
    "    if _use_multi_head():\n",
    "      placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "      labels_placeholder = {\n",
    "          _PRIMARY_HEAD: placeholder,\n",
    "          _SECONDARY_HEAD: placeholder,\n",
    "      }\n",
    "    else:\n",
    "      labels_placeholder = tf.compat.v1.placeholder(labels.dtype, labels.shape)\n",
    "    dataset = tf.data.Dataset.from_tensors(\n",
    "        (features_placeholder, labels_placeholder))\n",
    "    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "    if _use_multi_head():\n",
    "      feed_dict = {\n",
    "          labels_placeholder[head_name]: labels\n",
    "          for head_name in labels_placeholder\n",
    "      }\n",
    "    else:\n",
    "      feed_dict = {labels_placeholder: labels}\n",
    "    feed_dict.update(\n",
    "        {features_placeholder[k]: features[k] for k in features_placeholder})\n",
    "    iterator_initializer_hook.iterator_initializer_fn = (\n",
    "        lambda sess: sess.run(iterator.initializer, feed_dict=feed_dict))\n",
    "    return iterator.get_next()\n",
    "\n",
    "  return _eval_input_fn, iterator_initializer_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "predictions = []# np.array([])\n",
    "for x in estimator.predict(input_fn=train_input_fn):\n",
    "    print(x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " results = estimator.predict(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/learning-machine-learning/introduction-to-tensorflow-estimators-part-1-39f9eb666bc7\n",
    "# https://towardsdatascience.com/an-advanced-example-of-tensorflow-estimators-part-1-3-c9ffba3bff03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
